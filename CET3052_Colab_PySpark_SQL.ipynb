{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FFnYZltvqLgt"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sabaripkumar/digipen/blob/main/CET3052_Colab_PySpark_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iox_ufgbqDXa"
      },
      "source": [
        "<h1><center>Introduction to DataFrame-based PySpark Functions</center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFnYZltvqLgt"
      },
      "source": [
        "## Objective\n",
        "The objective of this notebook is to:\n",
        "<li>Understand SQL operations. </li>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd6t0uFzuR4X"
      },
      "source": [
        "## Installing Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6apGVff5h4ca"
      },
      "source": [
        "Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt7ZS1_wGgjn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6461d2c7-c25a-452f-a676-759335078119"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=d2b66347d39ed2afb2c79e644be0a1c734e5821289e5e12ac5abc6526e22d930\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3x0ZRLxjMVr"
      },
      "source": [
        "Set Environment Variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR1zLBk1998Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f089e9f-209a-4bfb-b04e-7289c56f5785"
      },
      "source": [
        "# PySpark applications start with initializing SparkSession which is the entry\n",
        "# point of PySpark as below. In case of running it in PySpark shell via pyspark\n",
        "# executable, the shell automatically creates the session in the variable spark\n",
        "# for users.\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 0)\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
        "\n",
        "print(spark)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7e536425a950>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNPhsx8P2tUH"
      },
      "source": [
        "<a id='spark-sql'></a>\n",
        "## Spark SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHMvBBAh23cw"
      },
      "source": [
        "SQL has been around since the 1970s. A large number of developers have mastered it and familar with its usage. As Big Data came into the landscape, new computing platforms have been created but they all chose to support SQL.\n",
        "\n",
        ">Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations.\n",
        "\n",
        "Basically, what you need to know is that Spark SQL is used to execute SQL queries on big data. Spark SQL can also be used to read data from Hive tables and views.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/annesjyu/dataengr2023/main/bank_marketing_1000.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWkpBHs3Ggmg",
        "outputId": "368db501-22e0-4c70-f697-e0b4825fff6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-08 07:50:08--  https://raw.githubusercontent.com/annesjyu/dataengr2023/main/bank_marketing_1000.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 117439 (115K) [text/plain]\n",
            "Saving to: ‘bank_marketing_1000.csv’\n",
            "\n",
            "\rbank_marketing_1000   0%[                    ]       0  --.-KB/s               \rbank_marketing_1000 100%[===================>] 114.69K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2024-08-08 07:50:09 (12.5 MB/s) - ‘bank_marketing_1000.csv’ saved [117439/117439]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bank_filename = \"bank_marketing_1000.csv\"\n",
        "\n",
        "# Load data and create a Spark DataFrame\n",
        "df = spark.read.csv(bank_filename, header=True, sep=\",\")\n",
        "\n",
        "print(f\"type(df) = {type(df)}\")"
      ],
      "metadata": {
        "id": "c-tbAMC3HDGw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a1220a-b229-46dd-e2c0-9b511e3bfa1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(df) = <class 'pyspark.sql.dataframe.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create A Temp View"
      ],
      "metadata": {
        "id": "7CqdrhmZIdNR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2DaK9-D7QkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e654f62-a76f-4ab7-ea4d-b7271af8d5d4"
      },
      "source": [
        "# Register Temporary Table\n",
        "df.createOrReplaceTempView(\"marketing\")\n",
        "\n",
        "# Select count of data in table\n",
        "spark.sql(\"select count(*) as total_count from marketing\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|total_count|\n",
            "+-----------+\n",
            "|        999|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all data from temp table\n",
        "sql_df = spark.sql(\"select * from marketing limit 5\")\n",
        "\n",
        "sql_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJanAUBsH4pH",
        "outputId": "3f99f0f1-6481-4dd2-802e-22d69e873693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
            "|age|      job|marital|  education|default|housing|loan|  contact|month|day_of_week|campaign|pdays|previous|   poutcome|emp.var.rate|cons.price.idx|cons.conf.idx|euribor3m|nr.employed|  y|\n",
            "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
            "| 56|housemaid|married|   basic.4y|     no|     no|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 57| services|married|high.school|unknown|     no|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 37| services|married|high.school|     no|    yes|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 40|   admin.|married|   basic.6y|     no|     no|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 56| services|married|high.school|     no|     no| yes|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average age of customers\n",
        "spark.sql(\"select round(avg(age)) as avg_age from marketing\").show()\n",
        "\n",
        "# Calculate the average age according to job\n",
        "spark.sql(\"select job, round(avg(age)) as avg_age from marketing group by job\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv45_0wV-IYs",
        "outputId": "d3ec3f2e-b25b-4364-89b5-5f59e4316e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|avg_age|\n",
            "+-------+\n",
            "|   42.0|\n",
            "+-------+\n",
            "\n",
            "+-------------+-------+\n",
            "|          job|avg_age|\n",
            "+-------------+-------+\n",
            "|   management|   46.0|\n",
            "|      retired|   54.0|\n",
            "|      unknown|   47.0|\n",
            "|self-employed|   42.0|\n",
            "|      student|   32.0|\n",
            "|  blue-collar|   42.0|\n",
            "| entrepreneur|   45.0|\n",
            "|       admin.|   41.0|\n",
            "|   technician|   42.0|\n",
            "|     services|   39.0|\n",
            "|    housemaid|   45.0|\n",
            "|   unemployed|   42.0|\n",
            "+-------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Global View\n",
        "\n",
        "Temporary views in Spark SQL are **session-scoped** and will *disappear* if the session that creates it terminates. If you want to have a temporary view shared among all sessions and keep alive until the Spark application closes, you can create a **global temporary view**. The global temporary view is tied to a system-preserved database *global_temp*, and we must use the qualified name to refer to it, e.g. `SELECT * FROM. global_temp.view1`."
      ],
      "metadata": {
        "id": "po4PxuPqIbR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the global temporary view exists\n",
        "try:\n",
        "  # Drop the global temporary view to contain customers from service sector.\n",
        "  spark.catalog.dropGlobalTempView(\"global_marketing\")\n",
        "  print(f\"Global temporary view global_marketing has been dropped.\")\n",
        "  spark.sql(\"CREATE GLOBAL TEMPORARY VIEW global_marketing AS SELECT * FROM marketing WHERE job = 'services'\")\n",
        "except:\n",
        "  pass\n",
        "\n",
        "spark.sql(\"SELECT * FROM global_temp.global_marketing\").show()\n",
        "spark.sql(\"SELECT COUNT(*) FROM global_temp.global_marketing\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df_TqgUHIUPa",
        "outputId": "a3dac5c1-e22f-4b39-bb1b-25b5ea8a2986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global temporary view global_marketing has been dropped.\n",
            "+---+--------+--------+-------------------+-------+-------+----+---------+-----+-----------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
            "|age|     job| marital|          education|default|housing|loan|  contact|month|day_of_week|campaign|pdays|previous|   poutcome|emp.var.rate|cons.price.idx|cons.conf.idx|euribor3m|nr.employed|  y|\n",
            "+---+--------+--------+-------------------+-------+-------+----+---------+-----+-----------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
            "| 57|services| married|        high.school|unknown|     no|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 37|services| married|        high.school|     no|    yes|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 56|services| married|        high.school|     no|     no| yes|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 45|services| married|           basic.9y|unknown|     no|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 25|services|  single|        high.school|     no|    yes|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 25|services|  single|        high.school|     no|    yes|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 34|services| married|        high.school|     no|     no|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 45|services| married|        high.school|unknown|    yes|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 33|services| married|        high.school|unknown|    yes|  no|telephone|  may|        mon|       2|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 45|services| married|professional.course|     no|    yes|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 54|services| married|            unknown|     no|    yes|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 54|services| married|            unknown|     no|     no|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 43|services|  single|        high.school|unknown|     no|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 35|services|divorced|        high.school|     no|    yes|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 41|services| married|        high.school|     no|     no|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 34|services| married|        high.school|     no|    yes|  no|telephone|  may|        mon|       2|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 39|services|divorced|        high.school|unknown|    yes|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 59|services| married|        high.school|     no|     no|  no|telephone|  may|        mon|       2|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 39|services|divorced|        high.school|unknown|     no|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "| 38|services| married|        high.school|     no|    yes| yes|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
            "+---+--------+--------+-------------------+-------+-------+----+---------+-----+-----------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|     121|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}