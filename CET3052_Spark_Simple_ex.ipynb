{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sabaripkumar/digipen/blob/main/CET3052_Spark_Simple_ex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sales Analysis\n",
        "\n",
        "This code takes a Sales dataset, analyze it according to countries. The code tries to demo how to use PySpark."
      ],
      "metadata": {
        "id": "PqVimwUIcgCP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFTADu93dY_7",
        "outputId": "79db09a6-79c0-4e57-a382-f642d672630b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataFrame and Frequency Counter"
      ],
      "metadata": {
        "id": "wpWRHyYij7Bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/annesjyu/dataengr2023/main/Sales.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqsEOnvgd9_L",
        "outputId": "0996437a-8060-4696-eedf-ba8d2c025a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-07 13:24:55--  https://raw.githubusercontent.com/annesjyu/dataengr2023/main/Sales.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 65773 (64K) [text/plain]\n",
            "Saving to: ‘Sales.csv.1’\n",
            "\n",
            "\rSales.csv.1           0%[                    ]       0  --.-KB/s               \rSales.csv.1         100%[===================>]  64.23K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-08-07 13:24:55 (4.35 MB/s) - ‘Sales.csv.1’ saved [65773/65773]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Sales Analysis\").getOrCreate()\n",
        "\n",
        "# Set the truncate option to False to prevent column truncation\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 0)"
      ],
      "metadata": {
        "id": "aISzOA64d-0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read csv data into a DataFrame\n",
        "df = spark.read.csv(\"Sales.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "WBbliQbEezvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ary8Gl8eY15",
        "outputId": "b55aae23-c889-423d-8fa6-0b0f90cd97c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+--------+-----+------------+---------------+------------+----------------+--------------+----------------+---------------+-----------+----------+\n",
            "|Transaction_date| Product|Price|Payment_Type|           Name|        City|           State|       Country| Account_Created|     Last_Login|   Latitude| Longitude|\n",
            "+----------------+--------+-----+------------+---------------+------------+----------------+--------------+----------------+---------------+-----------+----------+\n",
            "|   1/2/2009 6:17|Product1| 1200|  Mastercard|       carolina|    Basildon|         England|United Kingdom|   1/2/2009 6:00|  1/2/2009 6:08|       51.5|-1.1166667|\n",
            "|  1/3/2009 14:44|Product1| 1200|        Visa|          Gouya|      Echuca|        Victoria|     Australia| 9/25/2005 21:13| 1/3/2009 14:22|-36.1333333|    144.75|\n",
            "|  1/4/2009 13:17|Product1| 1200|  Mastercard|Renee Elisabeth|    Tel Aviv|        Tel Aviv|        Israel|  1/4/2009 13:03| 1/4/2009 22:10| 32.0666667|34.7666667|\n",
            "|  1/4/2009 14:11|Product1| 1200|        Visa|          Aidan|      Chatou|   Ile-de-France|        France|   6/3/2008 4:22|  1/5/2009 1:17| 48.8833333|      2.15|\n",
            "|   1/5/2009 5:39|Product1| 1200|        Amex|          Heidi|   Eindhoven|   Noord-Brabant|   Netherlands|   1/5/2009 4:55|  1/5/2009 8:15|      51.45| 5.4666667|\n",
            "|   1/4/2009 1:05|Product1| 1200|      Diners|         Leanne| Julianstown|           Meath|       Ireland|   1/4/2009 0:00| 1/5/2009 13:36| 53.6772222|-6.3191667|\n",
            "|  1/5/2009 11:37|Product1| 1200|        Visa|          Janet|      Ottawa|         Ontario|        Canada|   1/5/2009 9:35| 1/5/2009 19:24| 45.4166667|     -75.7|\n",
            "|   1/6/2009 5:02|Product1| 1200|      Diners|        barbara|   Hyderabad|  Andhra Pradesh|         India|   1/6/2009 2:41|  1/6/2009 7:52| 17.3833333|78.4666667|\n",
            "|   1/6/2009 7:45|Product2| 3600|        Visa|         Sabine|      London|         England|United Kingdom|   1/6/2009 7:00|  1/6/2009 9:17|   51.52721|   0.14559|\n",
            "|  1/6/2009 12:56|Product1| 1200|        Visa|         Jeremy|  Manchester|         England|United Kingdom|  1/6/2009 10:58| 1/6/2009 13:31|       53.5|-2.2166667|\n",
            "|  1/1/2009 11:05|Product1| 1200|      Diners|          Janis|   Ballynora|            Cork|       Ireland|12/10/2007 12:37|  1/7/2009 1:52| 51.8630556|     -8.58|\n",
            "|   1/5/2009 4:10|Product1| 1200|  Mastercard|         Nicola|  Roodepoort|         Gauteng|  South Africa|   1/5/2009 2:33|  1/7/2009 5:13|-26.1666667|27.8666667|\n",
            "|   1/2/2009 1:11|Product1| 1200|  Mastercard|           Lena|      Kuopio|Ita-Suomen Laani|       Finland| 12/31/2008 2:48| 1/7/2009 10:20|       62.9|27.6833333|\n",
            "|   1/2/2009 2:57|Product1| 1200|        Visa|          chris|      London|         England|United Kingdom|   1/3/2008 7:23| 1/7/2009 13:14|   51.52721|   0.14559|\n",
            "|   1/8/2009 1:59|Product1| 1200|  Mastercard|         SYLVIA|     Vesenaz|          Geneve|   Switzerland|11/28/2007 11:56|  1/8/2009 7:20| 46.2333333|       6.2|\n",
            "|  1/5/2009 13:17|Product1| 1200|  Mastercard|      Stephanie|Badhoevedorp|   Noord-Holland|   Netherlands|  1/5/2009 12:45| 1/8/2009 11:45| 52.3333333| 4.7833333|\n",
            "|  1/5/2009 20:00|Product2| 3600|        Visa|          James|  Burpengary|      Queensland|     Australia|12/10/2008 19:53| 1/8/2009 17:58|-27.1666667|    152.95|\n",
            "|  1/3/2009 13:24|Product1| 1200|        Visa|   Mehmet Fatih|   Helsingor|   Frederiksborg|       Denmark|  1/3/2009 12:47| 1/9/2009 11:14| 56.0333333|12.6166667|\n",
            "|  1/9/2009 15:58|Product1| 1200|  Mastercard|           Sari|     Newbury|         England|United Kingdom|  1/9/2009 14:53|1/10/2009 13:16|       51.4|-1.3166667|\n",
            "|  1/3/2009 13:11|Product1| 1200|  Mastercard|         simone|   Kobenhavn|       Kobenhavn|       Denmark| 10/31/2008 0:31|1/10/2009 13:24| 55.6666667|12.5833333|\n",
            "+----------------+--------+-----+------------+---------------+------------+----------------+--------------+----------------+---------------+-----------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.summary().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MavqJN9tfB5C",
        "outputId": "7eb6d559-6bca-486b-9549-de592c4c2c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------+--------+------------------+------------+-----+------+------+--------------+---------------+---------------+------------------+-----------------+\n",
            "|summary|Transaction_date| Product|             Price|Payment_Type| Name|  City| State|       Country|Account_Created|     Last_Login|          Latitude|        Longitude|\n",
            "+-------+----------------+--------+------------------+------------+-----+------+------+--------------+---------------+---------------+------------------+-----------------+\n",
            "|  count|             535|     535|               535|         535|  535|   535|   534|           535|            535|            535|               535|              535|\n",
            "|   mean|            NULL|    NULL|1645.7943925233644|        NULL| NULL|  NULL|  NULL|          NULL|           NULL|           NULL| 40.31821572149535|3.421527447476634|\n",
            "| stddev|            NULL|    NULL| 1093.107481054373|        NULL| NULL|  NULL|  NULL|          NULL|           NULL|           NULL|25.992945434334157|61.66761893173215|\n",
            "|    min|  1/1/2009 10:06|Product1|              1200|        Amex|Aaron|Aardal|      |     Argentina|  1/1/2005 6:03|1/10/2009 13:16|           -41.465|     -134.7166667|\n",
            "|    25%|            NULL|    NULL|              1200|        NULL| NULL|  NULL|  NULL|          NULL|           NULL|           NULL|        43.9333333|       -6.1447222|\n",
            "|    50%|            NULL|    NULL|              1200|        NULL| NULL|  NULL|  NULL|          NULL|           NULL|           NULL|        50.0833333|              4.3|\n",
            "|    75%|            NULL|    NULL|              1200|        NULL| NULL|  NULL|  NULL|          NULL|           NULL|           NULL|        52.3333333|            11.65|\n",
            "|    max|   1/9/2009 8:35|Product3|              7500|        Visa|sunny|Zurich|Zurich|United Kingdom|  9/9/2008 9:08|  3/1/2009 7:28|             63.85|      174.7666667|\n",
            "+-------+----------------+--------+------------------+------------+-----+------+------+--------------+---------------+---------------+------------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZfYjgPNedOx",
        "outputId": "8ba02e3d-828b-4c23-a929-d94b5052c9cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.dataframe.DataFrame</b><br/>def __init__(jdf: JavaObject, sql_ctx: Union[&#x27;SQLContext&#x27;, &#x27;SparkSession&#x27;])</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py</a>A distributed collection of data grouped into named columns.\n",
              "\n",
              ".. versionadded:: 1.3.0\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
              "and can be created using various functions in :class:`SparkSession`:\n",
              "\n",
              "&gt;&gt;&gt; people = spark.createDataFrame([\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 40, &quot;name&quot;: &quot;Hyukjin Kwon&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 50},\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 50, &quot;name&quot;: &quot;Takuya Ueshin&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 100},\n",
              "...     {&quot;deptId&quot;: 2, &quot;age&quot;: 60, &quot;name&quot;: &quot;Xinrong Meng&quot;, &quot;gender&quot;: &quot;F&quot;, &quot;salary&quot;: 150},\n",
              "...     {&quot;deptId&quot;: 3, &quot;age&quot;: 20, &quot;name&quot;: &quot;Haejoon Lee&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 200}\n",
              "... ])\n",
              "\n",
              "Once created, it can be manipulated using the various domain-specific-language\n",
              "(DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
              "\n",
              "To select a column from the :class:`DataFrame`, use the apply method:\n",
              "\n",
              "&gt;&gt;&gt; age_col = people.age\n",
              "\n",
              "A more concrete example:\n",
              "\n",
              "&gt;&gt;&gt; # To create DataFrame using SparkSession\n",
              "... department = spark.createDataFrame([\n",
              "...     {&quot;id&quot;: 1, &quot;name&quot;: &quot;PySpark&quot;},\n",
              "...     {&quot;id&quot;: 2, &quot;name&quot;: &quot;ML&quot;},\n",
              "...     {&quot;id&quot;: 3, &quot;name&quot;: &quot;Spark SQL&quot;}\n",
              "... ])\n",
              "\n",
              "&gt;&gt;&gt; people.filter(people.age &gt; 30).join(\n",
              "...     department, people.deptId == department.id).groupBy(\n",
              "...     department.name, &quot;gender&quot;).agg({&quot;salary&quot;: &quot;avg&quot;, &quot;age&quot;: &quot;max&quot;}).show()\n",
              "+-------+------+-----------+--------+\n",
              "|   name|gender|avg(salary)|max(age)|\n",
              "+-------+------+-----------+--------+\n",
              "|     ML|     F|      150.0|      60|\n",
              "|PySpark|     M|       75.0|      50|\n",
              "+-------+------+-----------+--------+\n",
              "\n",
              "Notes\n",
              "-----\n",
              "A DataFrame should only be created as described above. It should not be directly\n",
              "created via using the constructor.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 80);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the frequency of each country\n",
        "country_frequency = df.groupBy(\"Country\").count()\n",
        "\n",
        "# Show the result\n",
        "country_frequency.show(n=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWys29joeLrn",
        "outputId": "8c6b47ed-2dca-4f41-b33c-9dc14fdf84ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----+\n",
            "|    Country|count|\n",
            "+-----------+-----+\n",
            "|     Russia|    1|\n",
            "|     Sweden|   13|\n",
            "|     Jersey|    1|\n",
            "|Philippines|    2|\n",
            "|   Malaysia|    1|\n",
            "|     Turkey|    6|\n",
            "|    Germany|   25|\n",
            "|     France|   27|\n",
            "|     Greece|    1|\n",
            "|  Argentina|    1|\n",
            "+-----------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Schema and DataFrame"
      ],
      "metadata": {
        "id": "iSUMGY3_j28m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Define schema for our data using DDL\n",
        "schema = \"Id INT, First STRING, Last STRING, Url STRING, Published STRING, Hits INT, Campaigns ARRAY<STRING>\"\n",
        "\n",
        "# Create our static data\n",
        "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\"LinkedIn\"]],\n",
        "        [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\",\"LinkedIn\"]],\n",
        "        [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\",\"twitter\", \"FB\", \"LinkedIn\"]],\n",
        "        [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\", \"FB\"]],\n",
        "        [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\",\"twitter\", \"FB\", \"LinkedIn\"]],\n",
        "        [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"]]]\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = (SparkSession.builder.appName(\"schema_example\").getOrCreate())\n",
        "\n",
        "# Create a DataFrame using the schema defined above\n",
        "blogs_df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Show the DataFrame; it should reflect our table above\n",
        "blogs_df.show()\n",
        "\n",
        "# Print the schema used by Spark to process the DataFrame\n",
        "print(blogs_df.printSchema())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFvxSWrhkeNZ",
        "outputId": "7f05aa6d-4a7c-420a-f87d-adcf430c0974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
            "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
            "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
            "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
            "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
            "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
            "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
            "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
            "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
            "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
            "\n",
            "root\n",
            " |-- Id: integer (nullable = true)\n",
            " |-- First: string (nullable = true)\n",
            " |-- Last: string (nullable = true)\n",
            " |-- Url: string (nullable = true)\n",
            " |-- Published: string (nullable = true)\n",
            " |-- Hits: integer (nullable = true)\n",
            " |-- Campaigns: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(blogs_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJy8kwOclQge",
        "outputId": "d3d4af47-f90b-4fc2-d72b-782a0607100e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rows"
      ],
      "metadata": {
        "id": "EfRvgldymCpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row"
      ],
      "metadata": {
        "id": "QFSflxq_mBJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row1 = Row(\"Matei Zaharia\", \"CA\")\n",
        "row2 = Row(\"Reynold Xin\", \"CA\")\n",
        "rows = [row1, row2]\n",
        "\n",
        "print(type(row1))\n",
        "\n",
        "authors_df = spark.createDataFrame(rows, [\"Authors\", \"State\"])\n",
        "\n",
        "print(type(authors_df))\n",
        "\n",
        "print(authors_df.show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqr55RPmE8r",
        "outputId": "8e647ada-85f9-4d06-e631-4955f2020417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.types.Row'>\n",
            "<class 'pyspark.sql.dataframe.DataFrame'>\n",
            "+-------------+-----+\n",
            "|      Authors|State|\n",
            "+-------------+-----+\n",
            "|Matei Zaharia|   CA|\n",
            "|  Reynold Xin|   CA|\n",
            "+-------------+-----+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}